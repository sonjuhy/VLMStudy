from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    PreTrainedModel,
    AutoTokenizer,
    get_cosine_schedule_with_warmup,
)
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torchvision import transforms
from dataloader.llava_dataloader import (
    LlavaStage3Dataset,
    get_blip_laion_cc_558k_dataloader,
)
from vision.vit_model import Projector, ViTEncoder
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from accelerate import Accelerator
from utils.enums.e_path import JSONPathEnum, ImagePathEnum, CheckPointPathEnum
from PIL import Image
from io import BytesIO
from peft import PeftModel

import os
import requests
import torch
import torch.nn as nn


class ImageNet1KVLM(nn.Module):
    def __init__(
        self,
        vit: ViTEncoder,
        llm_model: PreTrainedModel,
        llm_hidden_size: int = 4096,
    ):
        super().__init__()
        self.vit_encoder: ViTEncoder = vit
        self.llm: PreTrainedModel = llm_model
        self.llm_hidden_size: int = llm_hidden_size
        self.projector: Projector = Projector(
            input_size=768, projection_size=self.llm_hidden_size
        )

        # LLMì€ í•™ìŠµì—ì„œ ì œì™¸ (Frozen)
        for param in self.vit_encoder.parameters():
            param.requires_grad = False
        for param in self.llm.parameters():
            param.requires_grad = False

        # Projectorë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        for param in self.projector.parameters():
            param.requires_grad = True

    def forward(self, images, input_ids, attention_mask=None, labels=None):
        # 1. ViTì—ì„œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (Batch, 196, 768)
        dtype = self.llm.dtype
        images = images.to(device=images.device, dtype=dtype)
        with torch.no_grad():
            image_features = self.vit_encoder.extract_features(images)
            image_features = image_features.to(dtype)

        # 2. Projector ì°¨ì› ë³€í™˜ (Batch, 196, 4096)
        image_embeddings = self.projector(image_features)

        # 3. í…ìŠ¤íŠ¸ ì„ë² ë”© (Batch, Seq_Len, 4096)
        text_embeddings = self.llm.get_input_embeddings()(input_ids)

        # 4. ê²°í•© [Image; Text] (Batch, 196 + Seq_Len, 4096)
        inputs_embeds = torch.cat([image_embeddings, text_embeddings], dim=1)

        # 5. Labels ê¸¸ì´ ë§ì¶”ê¸°
        # ì´ë¯¸ì§€ í† í° ìœ„ì¹˜(196ê°œ)ì—ëŠ” ì •ë‹µì´ ì—†ìœ¼ë¯€ë¡œ -100(Ignore Index)ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.
        if labels is not None:
            # labels: [Batch, Seq_Len]
            labels = labels.to(device=images.device)
            device = labels.device
            batch_size = labels.shape[0]

            # ì´ë¯¸ì§€ í† í° ê°œìˆ˜ë§Œí¼ -100 ì±„ìš°ê¸°
            ignore_labels = torch.full((batch_size, 196), -100, device=device)
            # ìµœì¢… ê²°í•©: [Batch, 196 + Seq_Len]
            full_labels = torch.cat([ignore_labels, labels], dim=1)
        else:
            full_labels = None

        # 6. Attention Mask ê²°í•©
        if attention_mask is not None:
            batch_size = images.shape[0]
            # ì´ë¯¸ì§€ ì˜ì—­(196ê°œ)ì€ ëª¨ë‘ '1'ë¡œ ì±„ìš´ ë§ˆìŠ¤í¬ ìƒì„±
            image_mask = torch.ones(
                (batch_size, 196),
                device=attention_mask.device,
                dtype=attention_mask.dtype,
            )
            # [Batch, 196 + Seq_Len]
            full_attention_mask = torch.cat([image_mask, attention_mask], dim=1)
        else:
            full_attention_mask = None

        # 7. LLM í†µê³¼
        outputs = self.llm(
            inputs_embeds=inputs_embeds,
            attention_mask=full_attention_mask,
            labels=full_labels,
        )
        return outputs


def projector_train(
    model: nn.Module,
    train_path: str,
    valid_path: str,
    json_path: str,
    img_root: str,
    epochs: int = 1,
):
    if os.path.exists(train_path) is False:
        raise ValueError(f"Train path {train_path} does not exist.")
    if os.path.exists(valid_path) is False:
        raise ValueError(f"Valid path {valid_path} does not exist.")

    optimizer = AdamW(model.projector.parameters(), lr=2e-4, weight_decay=0.1)

    val_transform = transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    # 1 Epochë§Œ í•˜ë¯€ë¡œ Warmupì„ ì§§ê³  ê°•í•˜ê²Œ ê°€ì ¸ê°‘ë‹ˆë‹¤.
    train_loader = get_blip_laion_cc_558k_dataloader(
        model_name="upstage/SOLAR-10.7B-Instruct-v1.0",
        vis_processor=val_transform,
        json_path=json_path,
        img_root=img_root,
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=len(train_loader)
    )
    scaler = torch.GradScaler()

    model.train()

    for epoch in range(epochs):
        epoch_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")

        for batch in pbar:
            # ë°ì´í„° ë¡œë“œ (device ì´ë™)
            images = batch["image"].to("cuda", dtype=torch.bfloat16)
            input_ids = batch["input_ids"].to("cuda")
            labels = batch["labels"].to("cuda")
            # attention_mask = batch["attention_mask"].to("cuda")
            if "attention_mask" in batch:
                attention_mask = batch["attention_mask"].to("cuda")
            else:
                # íŒ¨ë”© í† í°ì´ ì•„ë‹Œ ë¶€ë¶„ì€ 1, íŒ¨ë”©ì¸ ë¶€ë¶„ì€ 0ìœ¼ë¡œ ìƒì„±
                # ë³´í†µ tokenizer.pad_token_idê°€ 0ì¸ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.
                # attention_mask = (input_ids != tokenizer.pad_token_id).long().to("cuda")
                attention_mask = torch.ones_like(input_ids).to("cuda")

            optimizer.zero_grad()

            # Mixed Precision í•™ìŠµ (Bfloat16 ì‚¬ìš© ê¶Œì¥)
            with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16):
                outputs = model(
                    images=images,
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels,
                )
                loss = outputs.loss

            # ì—­ì „íŒŒ
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.projector.parameters(), max_norm=1.0)
            optimizer.step()

            # scaler.scale(loss).backward()
            # scaler.step(optimizer)
            # scaler.update()

            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (Step ë‹¨ìœ„)
            scheduler.step()

            # ë¡œê·¸ ê¸°ë¡
            epoch_loss += loss.item()
            pbar.set_postfix({"loss": loss.item()})

        avg_loss = epoch_loss / len(train_loader)
        print(f"Epoch {epoch+1} ì™„ë£Œ. í‰ê·  Loss: {avg_loss:.4f}")

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (Projector ê°€ì¤‘ì¹˜ë§Œ ì €ì¥í•˜ì—¬ ìš©ëŸ‰ ì•„ë¼ê¸°)
        save_path = f"solar_projector_epoch_{epoch+1}.pth"
        torch.save(model.projector.state_dict(), save_path)
        print(f"Projector saved to {save_path}")

    print("Stage 2 Alignment Finished!")
    return model.projector


def projector_train_test():
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 1. ëª¨ë¸ ê²½ë¡œ ë° ì„¤ì •
    llm_id = "upstage/SOLAR-10.7B-Instruct-v1.0"
    vit_checkpoint = "./checkpoints/final_model/vit_imagenet_1k_checkpoint_epoch_99.pth"  # ìµœê³  ì„±ëŠ¥ ì—í¬í¬
    train_json = "/media/edint/64d115f7-57cc-417b-acf0-7738ac091615/Ivern/DataSets/VLMDatasets/LlavaJson/blip_laion_cc_sbu_558k.json"
    valid_json = "/media/edint/64d115f7-57cc-417b-acf0-7738ac091615/Ivern/DataSets/VLMDatasets/LlavaJson/llava_instruct_150k.json"  # ê²€ì¦ìš©ìœ¼ë¡œ í™œìš© ê°€ëŠ¥
    img_root = "/media/edint/64d115f7-57cc-417b-acf0-7738ac091615/Ivern/DataSets/VLMDatasets/images/558_images"  # ì´ë¯¸ì§€ê°€ ëª¨ì¸ ìƒìœ„ í´ë”
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"

    print("--- 1. Loading Vision Encoder (ViT-Base) ---")
    # ê¸°ì¡´ì— ì •ì˜í•˜ì‹  ViTEncoder í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    vit = ViTEncoder(
        img_size=224,
        patch_size=16,
        embedding_size=768,
        num_class=1000,
        num_heads=12,
        in_channels=3,
    )
    checkpoint = torch.load(vit_checkpoint, map_location="cpu")

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (dict í˜•íƒœì¸ì§€ ì§ì ‘ ì¸ìŠ¤í„´ìŠ¤ í˜•íƒœì¸ì§€ í™•ì¸ í•„ìš”)
    if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
        vit.load_state_dict(checkpoint["model_state_dict"])
    else:
        vit.load_state_dict(checkpoint)
    vit.cuda()

    print(f"--- 2. Loading Language Model (SOLAR-10.7B) ---")
    # A6000ì—ì„œ 10.7B ëª¨ë¸ì„ 8-bitë¡œ ë¡œë“œí•˜ì—¬ VRAM ì ˆì•½
    llm = AutoModelForCausalLM.from_pretrained(
        llm_id,
        load_in_8bit=True,
        device_map="auto",
        dtype=torch.bfloat16,
        trust_remote_code=True,
    )

    print("--- 3. Initializing ImageNet1KVLM Wrapper ---")
    model = ImageNet1KVLM(
        vit=vit, llm_model=llm, llm_hidden_size=llm.config.hidden_size
    ).to(device=device)
    for name, param in model.named_parameters():
        if "projector" in name:
            param.requires_grad = True
        else:
            param.requires_grad = False

    model.llm.config.use_cache = False
    model.llm.gradient_checkpointing_enable()

    print("--- 4. Starting Projector Alignment Training ---")

    final_projector = projector_train(
        model=model,
        train_path=train_json,
        valid_path=valid_json,
        json_path=train_json,
        img_root=img_root,
        epochs=1,  # Stage 2ëŠ” 1 ì—í¬í¬ë©´ ì¶©ë¶„
    )

    print("--- All Processes Completed Successfully! ---")


def stage3_train(epochs: int = 1):
    # 1. ì´ˆê¸°í™” ë° í™˜ê²½ ì„¤ì •
    accelerator = Accelerator(
        gradient_accumulation_steps=16
    )  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ accumulation ì‚¬ìš©
    device = accelerator.device

    model_id = "upstage/SOLAR-10.7B-Instruct-v1.0"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token  # Solar íŒ¨ë”© í† í° ì„¤ì •

    # 2. ëª¨ë¸ ë¡œë“œ ë° LoRA ì„¤ì •
    print("Solar LLM ë¡œë“œ ì¤‘...")
    llm = AutoModelForCausalLM.from_pretrained(
        model_id,
        load_in_8bit=True,
        dtype=torch.bfloat16,
        device_map={"": accelerator.process_index},
    )
    llm = prepare_model_for_kbit_training(llm)  # ì–‘ìí™” ìœ„í•œ ì˜µì…˜

    # LoRA ì„¤ì •: Solarì˜ í•µì‹¬ ë ˆì´ì–´ë“¤ì— ì–´ëŒ‘í„° ì¶”ê°€
    lora_config = LoraConfig(
        r=64,
        lora_alpha=128,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    llm = get_peft_model(llm, lora_config)

    # 3. VLM êµ¬ì¡° ê²°í•© ë° Stage 2 ê°€ì¤‘ì¹˜ ì´ì‹
    vit = ViTEncoder(
        img_size=224,
        patch_size=16,
        embedding_size=768,
        num_class=1000,
        num_heads=12,
        in_channels=3,
    )
    vit_encoder = prepare_model_for_kbit_training(vit)
    vlm_model = (
        ImageNet1KVLM(
            llm_model=llm,
            llm_hidden_size=llm.config.hidden_size,
            vit=vit_encoder,
        )
        .to(device)
        .to(dtype=torch.bfloat16)
    )
    vlm_model.llm.gradient_checkpointing_enable()

    for name, param in vlm_model.named_parameters():
        if "lora_" in name or "projector" in name:
            param.requires_grad = True

    trainable_params = sum(p.numel() for p in vlm_model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in vlm_model.parameters())

    if accelerator.is_main_process:
        print(f"--- í•™ìŠµ íŒŒë¼ë¯¸í„° ì²´í¬ ---")
        print(f"í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} ê°œ")
        print(f"ì „ì²´ íŒŒë¼ë¯¸í„°: {all_params:,} ê°œ")
        print(f"ë¹„ì¤‘: {100 * trainable_params / all_params:.2f}%")
        print(f"------------------------")

    print("Stage 2 Projector ê°€ì¤‘ì¹˜ ì´ì‹ ì¤‘...")
    projector_path = CheckPointPathEnum.SOLAR_PROJECTOR_STAGE_2.value
    if os.path.exists(projector_path):
        vlm_model.projector.load_state_dict(
            torch.load(projector_path, map_location="cpu")
        )

    # 4. ë°ì´í„°ì…‹ ì¤€ë¹„
    val_transform = transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )
    dataset = LlavaStage3Dataset(
        json_path=JSONPathEnum.LLAVA_1_5_MIX665K_CLEAN.value,
        img_root=ImagePathEnum.LLAVA_ALL_IMAGES.value,
        tokenizer=tokenizer,
        vis_processor=val_transform,  # Stage 2ì™€ ë™ì¼í•œ í”„ë¡œì„¸ì„œ
    )

    train_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=8)

    # 5. ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
    optimizer = torch.optim.AdamW(vlm_model.parameters(), lr=2e-5)
    lr_scheduler = get_cosine_schedule_with_warmup(
        optimizer, num_warmup_steps=500, num_training_steps=len(train_loader)
    )

    # 6. Accelerate ì¤€ë¹„ (ëª¨ë¸, ì˜µí‹°ë§ˆì´ì € ë“± ë°°ë¶„)
    vlm_model, optimizer, train_loader, lr_scheduler = accelerator.prepare(
        vlm_model, optimizer, train_loader, lr_scheduler
    )

    # 7. í•™ìŠµ ë£¨í”„
    vlm_model.train()
    for epoch in range(epochs):
        pbar = tqdm(
            enumerate(train_loader),
            total=len(train_loader),
            disable=not accelerator.is_local_main_process,
        )
        for step, batch in pbar:
            if step == 0 and accelerator.is_main_process:
                # DDP wrapperë¥¼ ë²—ê¸°ê³  ì‹¤ì œ íŒŒë¼ë¯¸í„°ì˜ íƒ€ì…ì„ í™•ì¸
                unwrapped_model = accelerator.unwrap_model(vlm_model)
                # LLM ì„ë² ë”© ë ˆì´ì–´ì˜ íƒ€ì…ì„ í™•ì¸í•˜ëŠ” ê²ƒì´ ê°€ì¥ í™•ì‹¤í•©ë‹ˆë‹¤.
                current_dtype = unwrapped_model.llm.dtype

                print(f"--- ë””ë²„ê¹… ì •ë³´ ---")
                print(
                    f"ì´ë¯¸ì§€ í…ì„œ ëª¨ì–‘: {batch['image'].shape}"
                )  # [batch, 3, 224, 224]
                print(f"ì´ë¯¸ì§€ í…ì„œ dtype: {batch['image'].dtype}")
                print(f"ì¸í’‹ ì•„ì´ë”” ëª¨ì–‘: {batch['input_ids'].shape}")
                print(f"ëª¨ë¸(LLM) dtype: {current_dtype}")
            with accelerator.accumulate(vlm_model):
                # outputs = vlm_model(
                #     images=batch["image"],
                #     input_ids=batch["input_ids"],
                #     labels=batch["labels"],
                # )
                outputs = vlm_model(
                    images=batch["image"],
                    input_ids=batch["input_ids"],
                    attention_mask=batch.get("attention_mask"),  # ë§ˆìŠ¤í¬ê°€ ìˆë‹¤ë©´ ì „ë‹¬
                    labels=batch["labels"],
                )
                loss = outputs.loss
                accelerator.backward(loss)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            pbar.set_description(f"Epoch {epoch} | Loss: {loss.item():.4f}")

            # 8. ì¤‘ê°„ ì €ì¥ (5000 ìŠ¤í…ë§ˆë‹¤)
            if step % 5000 == 0 and step > 0:
                accelerator.wait_for_everyone()
                if accelerator.is_main_process:
                    save_dir = f"checkpoints/vlm/stage3/step_{step}"
                    os.makedirs(save_dir, exist_ok=True)

                    # LoRA ê°€ì¤‘ì¹˜ ì €ì¥
                    unwrapped_model = accelerator.unwrap_model(vlm_model)
                    unwrapped_model.llm.save_pretrained(save_dir)

                    # Projector ê°€ì¤‘ì¹˜ ë³„ë„ ì €ì¥
                    torch.save(
                        unwrapped_model.projector.state_dict(),
                        os.path.join(save_dir, "projector.bin"),
                    )
                    print(f"ğŸ’¾ Step {step} ì €ì¥ ì™„ë£Œ")

    # ìµœì¢… ì €ì¥
    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        save_dir = "checkpoints/vlm/stage3/final_model"
        os.makedirs(save_dir, exist_ok=True)

        # 1. ë¶„ì‚° í•™ìŠµ í™˜ê²½ì—ì„œ ëª¨ë¸ êº¼ë‚´ê¸°
        unwrapped_model = accelerator.unwrap_model(vlm_model)

        # 2. LLM ë¶€ë¶„(LoRA) ì €ì¥ (í´ë” ê²½ë¡œë§Œ ì§€ì •)
        unwrapped_model.llm.save_pretrained(save_dir)

        # 3. Projector ë¶€ë¶„ ì €ì¥
        torch.save(
            unwrapped_model.projector.state_dict(),
            os.path.join(save_dir, "projector.bin"),
        )

        # 4. í† í¬ë‚˜ì´ì €ë„ í•¨ê»˜ ì €ì¥
        tokenizer.save_pretrained(save_dir)

        print(f"ëª¨ë“  ëª¨ë¸ êµ¬ì„± ìš”ì†Œê°€ {save_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def get_image_from_url(url):
    try:
        # 1. URLë¡œë¶€í„° ì´ë¯¸ì§€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url, timeout=10)
        response.raise_for_status()  # 200 OKê°€ ì•„ë‹ˆë©´ ì—ëŸ¬ ë°œìƒ

        # 2. ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ ê°ì²´ë¡œ ë³€í™˜
        image = Image.open(BytesIO(response.content)).convert("RGB")
        return image
    except Exception as e:
        print(f"ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return None


def test_vlm_stage3(img_url: str, prompt_text: str, checkpoint_dir: str) -> None:
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    model_id: str = "upstage/SOLAR-10.7B-Instruct-v1.0"

    # ì›ë³¸ ViT ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (í•™ìŠµ ë•Œ ì¼ë˜ ì›ë³¸)
    vit_checkpoint: str = (
        "./checkpoints/final_model/vit_imagenet_1k_checkpoint_epoch_99.pth"
    )
    # ìƒˆë¡œ í•™ìŠµëœ Projector ê²½ë¡œ (LoRAì™€ ê°™ì€ í´ë”ì— ì €ì¥ë¨)
    proj_path: str = os.path.join(checkpoint_dir, "projector.bin")

    print("--- 1. í† í¬ë‚˜ì´ì € ë° LLM(LoRA ê²°í•©) ë¡œë“œ ì¤‘ ---")
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)  # ì €ì¥ëœ í† í¬ë‚˜ì´ì € ì‚¬ìš©

    # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
    )

    # â­ï¸ í•µì‹¬: í•™ìŠµëœ LoRA ê°€ì¤‘ì¹˜ ë³‘í•©
    print(f"âœ… LoRA ì–´ëŒ‘í„° ë¡œë“œ: {checkpoint_dir}")
    model = PeftModel.from_pretrained(base_model, checkpoint_dir)
    model.eval()

    print("--- 2. ë¹„ì „ ì¸ì½”ë” ë° Projector ë¡œë“œ ì¤‘ ---")
    vit = ViTEncoder(
        img_size=224, patch_size=16, embedding_size=768, num_class=1000, in_channels=3
    ).to(device, dtype=torch.bfloat16)

    projector = Projector(
        input_size=768, projection_size=base_model.config.hidden_size
    ).to(device, dtype=torch.bfloat16)

    # ViT ì›ë³¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    vit_ckpt = torch.load(vit_checkpoint, map_location=device)
    vit.load_state_dict(
        vit_ckpt["model_state_dict"] if "model_state_dict" in vit_ckpt else vit_ckpt
    )

    # â­ï¸ í•µì‹¬: ìƒˆë¡œ í•™ìŠµëœ Projector ê°€ì¤‘ì¹˜ ë¡œë“œ
    if os.path.exists(proj_path):
        projector.load_state_dict(torch.load(proj_path, map_location=device))
        print(f"âœ… Projector ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
    else:
        raise FileNotFoundError(f"Projector íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {proj_path}")

    vit.eval()
    projector.eval()

    print("--- 3. ì´ë¯¸ì§€ ë° í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì¤‘ ---")
    image: Image.Image | None = get_image_from_url(img_url)
    if image is None:
        return

    transform = transforms.Compose(
        [
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )
    image_tensor: torch.Tensor = (
        transform(image).unsqueeze(0).to(device, dtype=torch.bfloat16)
    )

    # â­ï¸ í”„ë¡¬í”„íŠ¸: ì´ë²ˆì—ëŠ” ê°•ì œë¡œ ìœ ë„í•˜ì§€ ì•Šê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ ë„í•©ë‹ˆë‹¤.
    full_prompt: str = f"### User: <image>\n{prompt_text}\n\n### Assistant:"

    print("--- 4. ë‹µë³€ ìƒì„± ì¤‘ ---")
    with torch.no_grad():
        image_features: torch.Tensor = vit.extract_features(image_tensor)
        image_embeddings: torch.Tensor = projector(image_features)

        text_inputs = tokenizer(
            full_prompt, return_tensors="pt", add_special_tokens=False
        ).to(device)
        text_embeddings: torch.Tensor = model.get_base_model().get_input_embeddings()(
            text_inputs.input_ids
        )

        bos_token_id: torch.Tensor = torch.tensor(
            [[tokenizer.bos_token_id]], device=device
        )
        bos_embeds: torch.Tensor = model.get_base_model().get_input_embeddings()(
            bos_token_id
        )

        inputs_embeds: torch.Tensor = torch.cat(
            [bos_embeds, image_embeddings, text_embeddings], dim=1
        )

        # Attention Mask
        image_mask: torch.Tensor = torch.ones(
            (1, 196), device=device, dtype=text_inputs.attention_mask.dtype
        )
        full_attention_mask = torch.ones(
            (1, inputs_embeds.shape[1]), device=device, dtype=torch.long
        )

        # â­ï¸ ìƒì„± ì˜µì…˜ ì¡°ì • (ëŒ€í™”í˜• ëª¨ë¸ì— ë§ê²Œ ì„¤ì •)
        output_ids = model.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=full_attention_mask,
            max_new_tokens=256,
            do_sample=False,  # ë‹µë³€ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ Greedy ì‚¬ìš©
            repetition_penalty=1.1,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            use_cache=True,
        )

    response: str = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    print("-" * 30)
    print(f"[ì§ˆë¬¸]: {prompt_text}")
    print(f"[ë‹µë³€]:\n{response.strip()}")


def validate_vlm(
    model: nn.Module,
    val_loader: DataLoader,
    accelerator: Accelerator,
    max_steps: int = None,
) -> float:
    """
    VLM ëª¨ë¸ì˜ Validation Lossë¥¼ ê³„ì‚°í•˜ëŠ” ì „ìš© í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        model (nn.Module): í•™ìŠµ ì¤‘ì¸ VLM ëª¨ë¸ (ImageNet1KVLM Wrapper)
        val_loader (DataLoader): ê²€ì¦ìš© ë°ì´í„°ë¡œë”
        accelerator (Accelerator): ë©€í‹° GPU ì²˜ë¦¬ë¥¼ ìœ„í•œ Accelerate ê°ì²´
        max_steps (int, optional): ì „ì²´ ê²€ì¦ì…‹ì´ ë„ˆë¬´ í´ ê²½ìš°, í™•ì¸í•  ìŠ¤í… ìˆ˜ ì œí•œ

    Returns:
        float: ê³„ì‚°ëœ í‰ê·  Validation Loss
    """
    # 1. í‰ê°€ ëª¨ë“œ ì „í™˜ (ë§¤ìš° ì¤‘ìš”: Dropout ë“± ë¹„í™œì„±í™”)
    model.eval()
    total_loss: float = 0.0
    total_steps: int = 0

    # ì§„í–‰ë¥  í‘œì‹œ (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ì¶œë ¥ë˜ë„ë¡ ì„¤ì •)
    pbar = tqdm(
        enumerate(val_loader),
        total=max_steps if max_steps else len(val_loader),
        desc="Validation",
        disable=not accelerator.is_local_main_process,
    )

    # 2. ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™” (VRAM ì ˆì•½ ë° ì†ë„ í–¥ìƒ)
    with torch.no_grad():
        for step, batch in pbar:
            if max_steps is not None and step >= max_steps:
                break

            # 3. ëª¨ë¸ Forward
            # (accelerator.prepare()ë¥¼ ê±°ì¹œ ë°ì´í„°ë¡œë”ì´ë¯€ë¡œ device ì´ë™ì€ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨)
            outputs = model(
                images=batch["image"],
                input_ids=batch["input_ids"],
                attention_mask=batch.get("attention_mask"),
                labels=batch["labels"],
            )

            loss: torch.Tensor = outputs.loss

            # 4. ë©€í‹° GPU í™˜ê²½ì—ì„œ Loss ë³‘í•© (Gather)
            # ê° GPUì—ì„œ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°ëœ loss í…ì„œë“¤ì„ í•˜ë‚˜ë¡œ ëª¨ì•„ì„œ í‰ê· ì„ ëƒ…ë‹ˆë‹¤.
            gathered_loss: torch.Tensor = accelerator.gather(loss)
            total_loss += gathered_loss.mean().item()
            total_steps += 1

            # í˜„ì¬ ìŠ¤í…ì˜ lossë¥¼ í”„ë¡œê·¸ë ˆìŠ¤ ë°”ì— í‘œì‹œ
            pbar.set_postfix({"val_loss": f"{loss.item():.4f}"})

    # 5. í‰ê·  Val Loss ê³„ì‚°
    avg_val_loss: float = total_loss / total_steps if total_steps > 0 else 0.0

    # 6. ì›ë˜ ìƒíƒœ(í•™ìŠµ ëª¨ë“œ)ë¡œ ë³µê·€
    model.train()

    return avg_val_loss


def run_standalone_validation() -> None:
    """
    ì €ì¥ëœ VLM ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ Validation Lossë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
    """
    # 1. í™˜ê²½ ì„¤ì • ë° Accelerator ì´ˆê¸°í™”
    # í‰ê°€ë§Œ í•˜ë¯€ë¡œ gradient_accumulationì€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    accelerator = Accelerator()
    device: torch.device = accelerator.device

    print("--- 1. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ ì¤‘ ---")
    model_id: str = "upstage/SOLAR-10.7B-Instruct-v1.0"
    checkpoint_dir: str = "checkpoints/vlm/stage3/final_model"  # ì¸¡ì •í•  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ

    tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)
    tokenizer.pad_token = tokenizer.eos_token

    # LLM ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ bfloat16 ì‚¬ìš©)
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map={"": accelerator.process_index},
        trust_remote_code=True,
    )

    # í•™ìŠµëœ LoRA ê°€ì¤‘ì¹˜ ë³‘í•©
    llm = PeftModel.from_pretrained(base_model, checkpoint_dir)

    # ë¹„ì „ ì¸ì½”ë” ë° ì´ˆê¸°í™”
    vit = ViTEncoder(
        img_size=224, patch_size=16, embedding_size=768, num_class=1000, in_channels=3
    ).to(dtype=torch.bfloat16)

    # 2. VLM Wrapper ëª¨ë¸ êµ¬ì„±
    vlm_model = (
        ImageNet1KVLM(
            llm_model=llm,
            llm_hidden_size=base_model.config.hidden_size,
            vit=vit,
        )
        .to(device)
        .to(dtype=torch.bfloat16)
    )

    # ViT ë° Projector ê°€ì¤‘ì¹˜ ë¡œë“œ
    vit_ckpt_path: str = (
        "./checkpoints/final_model/vit_imagenet_1k_checkpoint_epoch_99.pth"
    )
    proj_ckpt_path: str = os.path.join(checkpoint_dir, "projector.bin")

    vlm_model.vit_encoder.load_state_dict(
        torch.load(vit_ckpt_path, map_location="cpu")["model_state_dict"]
    )
    vlm_model.projector.load_state_dict(torch.load(proj_ckpt_path, map_location="cpu"))

    print("--- 2. ê²€ì¦ ë°ì´í„°ë¡œë” ì¤€ë¹„ ì¤‘ ---")
    val_transform = transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    valid_json = "/media/edint/64d115f7-57cc-417b-acf0-7738ac091615/Ivern/DataSets/VLMDatasets/LlavaJson/llava_instruct_150k.json"  # ê²€ì¦ìš©ìœ¼ë¡œ í™œìš© ê°€ëŠ¥
    img_root = "/media/edint/64d115f7-57cc-417b-acf0-7738ac091615/Ivern/DataSets/VLMDatasets/images/558_images"  # ì´ë¯¸ì§€ê°€ ëª¨ì¸ ìƒìœ„ í´ë”

    # ê²€ì¦ìš© JSON íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
    val_dataset = LlavaStage3Dataset(
        json_path=valid_json,  # ë¶„ë¦¬ëœ ê²€ì¦ ë°ì´í„° ê²½ë¡œ
        img_root=img_root,
        tokenizer=tokenizer,
        vis_processor=val_transform,
    )

    # í‰ê°€ ì‹œì—ëŠ” ì…”í”Œì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=4)

    # 3. Accelerator ì¤€ë¹„ (ëª¨ë¸ê³¼ ë°ì´í„°ë¡œë”ë¥¼ ë¶„ì‚° í™˜ê²½ì— ë§¤í•‘)
    vlm_model, val_loader = accelerator.prepare(vlm_model, val_loader)

    print("--- 3. Validation í‰ê°€ ì‹œì‘ ---")
    # ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ ëŒë©´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ max_steps ì„¤ì • (ì˜ˆ: 500ê°œ ìƒ˜í”Œë§Œ í™•ì¸)
    final_val_loss: float = validate_vlm(
        model=vlm_model, val_loader=val_loader, accelerator=accelerator, max_steps=500
    )

    if accelerator.is_main_process:
        print("=" * 40)
        print(f"âœ… ìµœì¢… ê²€ì¦ ì™„ë£Œ!")
        print(f"ğŸ“Š ì¸¡ì •í•œ ì²´í¬í¬ì¸íŠ¸: {checkpoint_dir}")
        print(f"ğŸ“‰ í‰ê·  Validation Loss: {final_val_loss:.4f}")
        print("=" * 40)


def end_to_end_test():
    pass
