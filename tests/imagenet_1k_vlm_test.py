from tqdm import tqdm
from transformers import (
    AutoModelForCausalLM,
    PreTrainedModel,
    AutoTokenizer,
    get_cosine_schedule_with_warmup,
)
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torchvision import transforms
from dataloader.llava_dataloader import (
    LlavaStage3Dataset,
    get_blip_laion_cc_558k_dataloader,
)
from vision.vit_model import Projector, ViTEncoder
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from accelerate import Accelerator
from utils.enums.e_path import JSONPathEnum, ImagePathEnum, CheckPointPathEnum

import os
import torch
import torch.nn as nn


class ImageNet1KVLM(nn.Module):
    def __init__(
        self,
        vit: ViTEncoder,
        llm_model: PreTrainedModel,
        llm_hidden_size: int = 4096,
    ):
        super().__init__()
        self.vit_encoder: ViTEncoder = vit
        self.llm: PreTrainedModel = llm_model
        self.llm_hidden_size: int = llm_hidden_size
        self.projector: Projector = Projector(
            input_size=768, projection_size=self.llm_hidden_size
        )

        # LLMì€ í•™ìŠµì—ì„œ ì œì™¸ (Frozen)
        for param in self.vit_encoder.parameters():
            param.requires_grad = False
        for param in self.llm.parameters():
            param.requires_grad = False

        # Projectorë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
        for param in self.projector.parameters():
            param.requires_grad = True

    def forward(self, images, input_ids, labels=None):
        # 1. ViTì—ì„œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ (Batch, 196, 768)
        dtype = self.llm.dtype
        images = images.to(device=images.device, dtype=dtype)
        with torch.no_grad():
            image_features = self.vit_encoder.extract_features(images)
            image_features = image_features.to(dtype)

        # 2. Projector ì°¨ì› ë³€í™˜ (Batch, 196, 4096)
        image_embeddings = self.projector(image_features)

        # 3. í…ìŠ¤íŠ¸ ì„ë² ë”© (Batch, Seq_Len, 4096)
        text_embeddings = self.llm.get_input_embeddings()(input_ids)

        # 4. ê²°í•© [Image; Text] (Batch, 196 + Seq_Len, 4096)
        inputs_embeds = torch.cat([image_embeddings, text_embeddings], dim=1)

        # 5. Labels ê¸¸ì´ ë§ì¶”ê¸°
        # ì´ë¯¸ì§€ í† í° ìœ„ì¹˜(196ê°œ)ì—ëŠ” ì •ë‹µì´ ì—†ìœ¼ë¯€ë¡œ -100(Ignore Index)ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.
        if labels is not None:
            # labels: [Batch, Seq_Len]
            labels = labels.to(device=images.device)
            device = labels.device
            batch_size = labels.shape[0]

            # ì´ë¯¸ì§€ í† í° ê°œìˆ˜ë§Œí¼ -100 ì±„ìš°ê¸°
            ignore_labels = torch.full((batch_size, 196), -100, device=device)
            # ìµœì¢… ê²°í•©: [Batch, 196 + Seq_Len]
            full_labels = torch.cat([ignore_labels, labels], dim=1)
        else:
            full_labels = None

        # 6. LLM í†µê³¼
        outputs = self.llm(inputs_embeds=inputs_embeds, labels=full_labels)
        return outputs


def projector_train(
    model: nn.Module,
    train_path: str,
    valid_path: str,
    json_path: str,
    img_root: str,
    epochs: int = 1,
):
    if os.path.exists(train_path) is False:
        raise ValueError(f"Train path {train_path} does not exist.")
    if os.path.exists(valid_path) is False:
        raise ValueError(f"Valid path {valid_path} does not exist.")

    optimizer = AdamW(model.projector.parameters(), lr=1e-3, weight_decay=0.1)

    val_transform = transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    # 1 Epochë§Œ í•˜ë¯€ë¡œ Warmupì„ ì§§ê³  ê°•í•˜ê²Œ ê°€ì ¸ê°‘ë‹ˆë‹¤.
    train_loader = get_blip_laion_cc_558k_dataloader(
        model_name="upstage/SOLAR-10.7B-Instruct-v1.0",
        vis_processor=val_transform,
        json_path=json_path,
        img_root=img_root,
    )
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=len(train_loader)
    )
    scaler = torch.GradScaler()

    model.train()

    for epoch in range(epochs):
        epoch_loss = 0
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")

        for batch in pbar:
            # ë°ì´í„° ë¡œë“œ (device ì´ë™)
            images = batch["image"].to("cuda", dtype=torch.bfloat16)
            input_ids = batch["input_ids"].to("cuda")
            labels = batch["labels"].to("cuda")

            optimizer.zero_grad()

            # Mixed Precision í•™ìŠµ (Bfloat16 ì‚¬ìš© ê¶Œì¥)
            with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16):
                outputs = model(images, input_ids, labels)
                loss = outputs.loss

            # ì—­ì „íŒŒ
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (Step ë‹¨ìœ„)
            scheduler.step()

            # ë¡œê·¸ ê¸°ë¡
            epoch_loss += loss.item()
            pbar.set_postfix({"loss": loss.item()})

        avg_loss = epoch_loss / len(train_loader)
        print(f"Epoch {epoch+1} ì™„ë£Œ. í‰ê·  Loss: {avg_loss:.4f}")

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (Projector ê°€ì¤‘ì¹˜ë§Œ ì €ì¥í•˜ì—¬ ìš©ëŸ‰ ì•„ë¼ê¸°)
        save_path = f"solar_projector_epoch_{epoch+1}.pth"
        torch.save(model.projector.state_dict(), save_path)
        print(f"Projector saved to {save_path}")

    print("Stage 2 Alignment Finished!")
    return model.projector


def projector_train_test():
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 1. ëª¨ë¸ ê²½ë¡œ ë° ì„¤ì •
    llm_id = "upstage/SOLAR-10.7B-Instruct-v1.0"
    vit_checkpoint = "./checkpoints/final_model/vit_imagenet_1k_checkpoint_epoch_99.pth"  # ìµœê³  ì„±ëŠ¥ ì—í¬í¬
    train_json = "/media/edint/64d115f7-57cc-417b-acf0-7738ac091615/Ivern/DataSets/VLMDatasets/LlavaJson/blip_laion_cc_sbu_558k.json"
    valid_json = "/media/edint/64d115f7-57cc-417b-acf0-7738ac091615/Ivern/DataSets/VLMDatasets/LlavaJson/llava_instruct_150k.json"  # ê²€ì¦ìš©ìœ¼ë¡œ í™œìš© ê°€ëŠ¥
    img_root = "/media/edint/64d115f7-57cc-417b-acf0-7738ac091615/Ivern/DataSets/VLMDatasets/images/558_images"  # ì´ë¯¸ì§€ê°€ ëª¨ì¸ ìƒìœ„ í´ë”
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"

    print("--- 1. Loading Vision Encoder (ViT-Base) ---")
    # ê¸°ì¡´ì— ì •ì˜í•˜ì‹  ViTEncoder í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    vit = ViTEncoder(
        img_size=224,
        patch_size=16,
        embedding_size=768,
        num_class=1000,
        num_heads=12,
        in_channels=3,
    )
    checkpoint = torch.load(vit_checkpoint, map_location="cpu")

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (dict í˜•íƒœì¸ì§€ ì§ì ‘ ì¸ìŠ¤í„´ìŠ¤ í˜•íƒœì¸ì§€ í™•ì¸ í•„ìš”)
    if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
        vit.load_state_dict(checkpoint["model_state_dict"])
    else:
        vit.load_state_dict(checkpoint)
    vit.cuda()

    print(f"--- 2. Loading Language Model (SOLAR-10.7B) ---")
    # A6000ì—ì„œ 10.7B ëª¨ë¸ì„ 8-bitë¡œ ë¡œë“œí•˜ì—¬ VRAM ì ˆì•½
    llm = AutoModelForCausalLM.from_pretrained(
        llm_id,
        load_in_8bit=True,
        device_map="auto",
        dtype=torch.bfloat16,
        trust_remote_code=True,
    )

    print("--- 3. Initializing ImageNet1KVLM Wrapper ---")
    model = ImageNet1KVLM(
        vit=vit, llm_model=llm, llm_hidden_size=llm.config.hidden_size
    ).to(device=device)
    model.llm.gradient_checkpointing_enable()

    print("--- 4. Starting Projector Alignment Training ---")

    final_projector = projector_train(
        model=model,
        train_path=train_json,
        valid_path=valid_json,
        json_path=train_json,
        img_root=img_root,
        epochs=1,  # Stage 2ëŠ” 1 ì—í¬í¬ë©´ ì¶©ë¶„
    )

    print("--- All Processes Completed Successfully! ---")


def stage3_train(epochs: int = 1):
    # 1. ì´ˆê¸°í™” ë° í™˜ê²½ ì„¤ì •
    accelerator = Accelerator(
        gradient_accumulation_steps=16
    )  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ accumulation ì‚¬ìš©
    device = accelerator.device

    model_id = "upstage/SOLAR-10.7B-Instruct-v1.0"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.eos_token  # Solar íŒ¨ë”© í† í° ì„¤ì •

    # 2. ëª¨ë¸ ë¡œë“œ ë° LoRA ì„¤ì •
    print("Solar LLM ë¡œë“œ ì¤‘...")
    llm = AutoModelForCausalLM.from_pretrained(
        model_id,
        load_in_8bit=True,
        dtype=torch.bfloat16,
        device_map={"": device},
    )
    llm = prepare_model_for_kbit_training(llm)  # ì–‘ìí™” ìœ„í•œ ì˜µì…˜

    # LoRA ì„¤ì •: Solarì˜ í•µì‹¬ ë ˆì´ì–´ë“¤ì— ì–´ëŒ‘í„° ì¶”ê°€
    lora_config = LoraConfig(
        r=64,
        lora_alpha=128,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )
    llm = get_peft_model(llm, lora_config)

    # 3. VLM êµ¬ì¡° ê²°í•© ë° Stage 2 ê°€ì¤‘ì¹˜ ì´ì‹
    vit = ViTEncoder(
        img_size=224,
        patch_size=16,
        embedding_size=768,
        num_class=1000,
        num_heads=12,
        in_channels=3,
    )
    vit_encoder = prepare_model_for_kbit_training(vit)
    vlm_model = (
        ImageNet1KVLM(
            llm_model=llm,
            llm_hidden_size=llm.config.hidden_size,
            vit=vit_encoder,
        )
        .to(device)
        .to(dtype=torch.bfloat16)
    )
    vlm_model.llm.gradient_checkpointing_enable()

    for name, param in vlm_model.named_parameters():
        if "lora_" in name or "projector" in name:
            param.requires_grad = True

    trainable_params = sum(p.numel() for p in vlm_model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in vlm_model.parameters())

    if accelerator.is_main_process:
        print(f"--- í•™ìŠµ íŒŒë¼ë¯¸í„° ì²´í¬ ---")
        print(f"í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} ê°œ")
        print(f"ì „ì²´ íŒŒë¼ë¯¸í„°: {all_params:,} ê°œ")
        print(f"ë¹„ì¤‘: {100 * trainable_params / all_params:.2f}%")
        print(f"------------------------")

    print("Stage 2 Projector ê°€ì¤‘ì¹˜ ì´ì‹ ì¤‘...")
    projector_path = CheckPointPathEnum.SOLAR_PROJECTOR_STAGE_2.value
    if os.path.exists(projector_path):
        vlm_model.projector.load_state_dict(
            torch.load(projector_path, map_location="cpu")
        )

    # 4. ë°ì´í„°ì…‹ ì¤€ë¹„
    val_transform = transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )
    dataset = LlavaStage3Dataset(
        json_path=JSONPathEnum.LLAVA_1_5_MIX665K_CLEAN.value,
        img_root=ImagePathEnum.LLAVA_ALL_IMAGES.value,
        tokenizer=tokenizer,
        vis_processor=val_transform,  # Stage 2ì™€ ë™ì¼í•œ í”„ë¡œì„¸ì„œ
    )

    train_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=8)

    # 5. ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
    optimizer = torch.optim.AdamW(vlm_model.parameters(), lr=2e-5)
    lr_scheduler = get_cosine_schedule_with_warmup(
        optimizer, num_warmup_steps=100, num_training_steps=len(train_loader)
    )

    # 6. Accelerate ì¤€ë¹„ (ëª¨ë¸, ì˜µí‹°ë§ˆì´ì € ë“± ë°°ë¶„)
    vlm_model, optimizer, train_loader, lr_scheduler = accelerator.prepare(
        vlm_model, optimizer, train_loader, lr_scheduler
    )

    # 7. í•™ìŠµ ë£¨í”„
    vlm_model.train()
    for epoch in range(epochs):
        pbar = tqdm(
            enumerate(train_loader),
            total=len(train_loader),
            disable=not accelerator.is_local_main_process,
        )
        for step, batch in pbar:
            if step == 0 and accelerator.is_main_process:
                # DDP wrapperë¥¼ ë²—ê¸°ê³  ì‹¤ì œ íŒŒë¼ë¯¸í„°ì˜ íƒ€ì…ì„ í™•ì¸
                unwrapped_model = accelerator.unwrap_model(vlm_model)
                # LLM ì„ë² ë”© ë ˆì´ì–´ì˜ íƒ€ì…ì„ í™•ì¸í•˜ëŠ” ê²ƒì´ ê°€ì¥ í™•ì‹¤í•©ë‹ˆë‹¤.
                current_dtype = unwrapped_model.llm.dtype

                print(f"--- ë””ë²„ê¹… ì •ë³´ ---")
                print(
                    f"ì´ë¯¸ì§€ í…ì„œ ëª¨ì–‘: {batch['image'].shape}"
                )  # [batch, 3, 224, 224]
                print(f"ì´ë¯¸ì§€ í…ì„œ dtype: {batch['image'].dtype}")
                print(f"ì¸í’‹ ì•„ì´ë”” ëª¨ì–‘: {batch['input_ids'].shape}")
                print(f"ëª¨ë¸(LLM) dtype: {current_dtype}")
            with accelerator.accumulate(vlm_model):
                outputs = vlm_model(
                    images=batch["image"],
                    input_ids=batch["input_ids"],
                    labels=batch["labels"],
                )
                loss = outputs.loss
                accelerator.backward(loss)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            pbar.set_description(f"Epoch {epoch} | Loss: {loss.item():.4f}")

            # 8. ì¤‘ê°„ ì €ì¥ (5000 ìŠ¤í…ë§ˆë‹¤)
            if step % 5000 == 0 and step > 0:
                accelerator.wait_for_everyone()
                if accelerator.is_main_process:
                    save_dir = f"checkpoints/vlm/stage3/step_{step}"
                    os.makedirs(save_dir, exist_ok=True)

                    # LoRA ê°€ì¤‘ì¹˜ ì €ì¥
                    unwrapped_model = accelerator.unwrap_model(vlm_model)
                    unwrapped_model.llm.save_pretrained(save_dir)

                    # Projector ê°€ì¤‘ì¹˜ ë³„ë„ ì €ì¥
                    torch.save(
                        unwrapped_model.projector.state_dict(),
                        os.path.join(save_dir, "projector.bin"),
                    )
                    print(f"ğŸ’¾ Step {step} ì €ì¥ ì™„ë£Œ")

    # ìµœì¢… ì €ì¥
    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        save_dir = "checkpoints/vlm/stage3/final_model"
        os.makedirs(save_dir, exist_ok=True)

        # 1. ë¶„ì‚° í•™ìŠµ í™˜ê²½ì—ì„œ ëª¨ë¸ êº¼ë‚´ê¸°
        unwrapped_model = accelerator.unwrap_model(vlm_model)

        # 2. LLM ë¶€ë¶„(LoRA) ì €ì¥ (í´ë” ê²½ë¡œë§Œ ì§€ì •)
        unwrapped_model.llm.save_pretrained(save_dir)

        # 3. Projector ë¶€ë¶„ ì €ì¥
        torch.save(
            unwrapped_model.projector.state_dict(),
            os.path.join(save_dir, "projector.bin"),
        )

        # 4. í† í¬ë‚˜ì´ì €ë„ í•¨ê»˜ ì €ì¥
        tokenizer.save_pretrained(save_dir)

        print(f"ëª¨ë“  ëª¨ë¸ êµ¬ì„± ìš”ì†Œê°€ {save_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def end_to_end_test():
    pass
